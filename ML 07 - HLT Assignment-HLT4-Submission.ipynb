{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLT Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to practising the process of loading data, regularizing it and using it to train a model, the goal of this assignment is to investigat the effect of changing model parameters on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be going a bit further with the robot collision dataset. This time, instead of looking at just the first file, we'll look at all five different tasks combined (lp1.data to lp5.data). Prepare two different arrays, X1 and X2, as follows:\n",
    "- Each element in X1 is the immediate reading of the force and torque values after an event, [f1, f2, f3, t1, t2, t3]. The first element should be [1, 1, 63, 3, 1, 0]\n",
    "- Each element in X2 contains 18 values in total - the first, fifth and tenth sets of sensor readings after an event. The first element should be [-1, -1, 61, -3, 0, 0, -1, -1, 63, -3, -1, 0, -1, -1, 61, -3, 0, 0]\n",
    "\n",
    "y should contain the corresponding classes, represented as integers according the the provided dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['robot_execution_failure/lp1.data', 'robot_execution_failure/lp2.data',\n",
    "'robot_execution_failure/lp3.data', 'robot_execution_failure/lp4.data', 'robot_execution_failure/lp5.data']\n",
    "classes = {'normal':0, 'collision':1, 'obstruction':2, 'fr_collision':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "X1 = [] \n",
    "X2 = []\n",
    "\n",
    "# true values\n",
    "y = []\n",
    "\n",
    "for i in range(len(files) - 1):\n",
    "    filename = files[i]\n",
    "    f = open(filename)\n",
    "    lines = f.readlines() # Read the file line by line into a list\n",
    "    for i in range(len(lines) - 1):\n",
    "        line = lines[i].strip() # .strip() removes the line endings \\n\n",
    "        if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "            featuresX1 = [int(x) for x in lines[i+1].strip().split('\\t')]\n",
    "            temp = lines[i+1].strip().split('\\t') + lines[i+5].strip().split('\\t')+ lines[i+10].strip().split('\\t')\n",
    "            featuresX2 = [int(x) for x in temp]\n",
    "            X1.append(featuresX1)\n",
    "            X2.append(featuresX2)\n",
    "            y.append(classes[line]) # And record which class this set of features belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 63, -3, -1, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 63, -3, -1, 0, -1, -1, 63, -3, -1, 0, -1, -1, 61, -3, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take note that the first entry of X1 and X2 do not match the suggested values in the task. \n",
    "#        After checking the actual files, It was determined that the values obtained in X1[0] and X2[0] \n",
    "#        above are correct as per the actual data file contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: establishing a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using techniques covered in this unit, split X1 and y into separate training and testing sets. Use the training set to train a neural network (MLPClassifier) using default parameters but with hidden_layer_sizes=(20, 20, 20). Use the test data you held back to score the model you have created. How well does it perform? Print out the score and confusion matrix. For more accuracy, run through these steps 10 times and find the average score - bonus points for running more times and getting a standard deviation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing to scale the inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the inputs \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7017543859649122\n"
     ]
    }
   ],
   "source": [
    "# Predict and print score\n",
    "predictions = mlp.predict(X_test)\n",
    "print (mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're getting a convergence warning, you could try having the model train over more iterations - change max_iter = 1000 or 10,000. Does this improve the average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Set max iterations to 1000\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter = 1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "print (mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6491228070175439\n"
     ]
    }
   ],
   "source": [
    "# Set max iterations to 10000\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter = 10000)\n",
    "mlp.fit(X_train, y_train)\n",
    "print (mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation:\n",
      "0.041692051362512784\n",
      "[[17  3  0  0]\n",
      " [ 1 13  4  0]\n",
      " [ 1  4  8  0]\n",
      " [ 2  0  1  3]]\n"
     ]
    }
   ],
   "source": [
    "# Initialise scores array\n",
    "scores = []\n",
    "\n",
    "# Repeat 20 times\n",
    "count = 0\n",
    "while count < 20:\n",
    "    mlp.fit(X_train, y_train) # Training\n",
    "    mlp.score(X_test, y_test)\n",
    "    scores.append(mlp.score(X_test, y_test))\n",
    "    count +=1\n",
    "    \n",
    "# Print the Standard Deviation of the scores\n",
    "import numpy as np\n",
    "print('Standard Deviation:')\n",
    "print(np.std(scores))\n",
    "\n",
    "predictions = mlp.predict(X_test)\n",
    "\n",
    "# Print the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: adding more inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use X2 in place of X1 - does the score increase or decrease? Was this what you expected? How many samples are there in our training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7192982456140351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Repeating the steps above with X2 instead of X1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20))\n",
    "mlp.fit(X_train, y_train)\n",
    "print (mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score is better with the added inputs. You would expect this since it has more dimensions to describe a condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to X1 as our input. Add an extra feature to each item in the array to represent the total force $f_t$. Assume:\n",
    "\n",
    "$f_t^2 = f_1^2 + f_2^2 + f_3^2$\n",
    "\n",
    "Your first input should now look like this:\n",
    "X1[0] = [-1, -1, 61, -3, 0, 0, 61.0163912403872]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the steps from step 2. *Has this extra feature improved model performance?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "X1 = [] \n",
    "\n",
    "# true values\n",
    "y = []\n",
    "\n",
    "for i in range(len(files) - 1):\n",
    "    filename = files[i]\n",
    "    f = open(filename)\n",
    "    lines = f.readlines() # Read the file line by line into a list\n",
    "    for i in range(len(lines) - 1):\n",
    "        line = lines[i].strip() # .strip() removes the line endings \\n\n",
    "        if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "            featuresX1 = [int(x) for x in lines[i+1].strip().split('\\t')]\n",
    "            X1.append(featuresX1)\n",
    "            y.append(classes[line]) # And record which class this set of features belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new feature and appending it to the dataframe\n",
    "for i in range(len(X1)):\n",
    "    totalForce = (((X1[i][0])**2)+((X1[i][1])**2)+((X1[i][2])**2))**(0.5)\n",
    "    X1[i].append(totalForce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 63, -3, -1, 0, 63.0158710167526]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7543859649122807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter = 1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "print (mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is observed that the new feature decreased the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create at least 3 more models, adding features or changing the number and size of the hidden layers. Print out the average score for your best model. Comment on what you've found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "X1 = [] \n",
    "\n",
    "# true values\n",
    "y = []\n",
    "\n",
    "for i in range(len(files) - 1):\n",
    "    filename = files[i]\n",
    "    f = open(filename)\n",
    "    lines = f.readlines() # Read the file line by line into a list\n",
    "    for i in range(len(lines) - 1):\n",
    "        line = lines[i].strip() # .strip() removes the line endings \\n\n",
    "        if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "            featuresX1 = [int(x) for x in lines[i+1].strip().split('\\t')]\n",
    "            X1.append(featuresX1)\n",
    "            y.append(classes[line]) # And record which class this set of features belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Model 1 (More hidden layers)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y)\n",
    "scaler = StandardScaler() # Preparing to scale the inputs\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(90,90,90))\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22807017543859648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Model 2 (More less layers)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5,5))\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Adding more features by adding the 1st, 4th, 8th, 12th and 15th sets of sensor readings after an event\n",
    "# (Excluding the total force)\n",
    "\n",
    "X1 = [] # inputs\n",
    "y  = [] # true values\n",
    "\n",
    "for i in range(len(lines) - 1):\n",
    "    line = lines[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        features = [int(x) for x in lines[i+1].strip().split('\\t')] # Split the next line to get our features\n",
    "        X1lines = lines[i+1] + lines[i+4] + lines[i+8] + lines[i+12] + lines[i+15]\n",
    "        features2 = [int(x) for x in X1lines.strip().split('\\t')] # Split the next line to get our features\n",
    "        X1.append(features2)\n",
    "        y.append(classes[line]) # And record which class this set of features belongs to        \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y)\n",
    "scaler = StandardScaler() # Preparing to scale the inputs\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20))\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
